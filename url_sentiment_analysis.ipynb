{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70569ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6753fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat                  \n",
    "import openpyxl                 \n",
    "import string                  \n",
    "import spacy                  \n",
    "import re   \n",
    "import requests                  \n",
    "import pandas as pd              \n",
    "from bs4 import BeautifulSoup    \n",
    "from textblob import TextBlob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7207f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\shara\\Downloads\\BlackCoffer/negative-words.txt\",\"r\",encoding = \"ISO-8859-1\") as neg:\n",
    "    negw = neg.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f01ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\shara\\Downloads\\BlackCoffer/positive-words.txt\",\"r\") as pos:\n",
    "    posw = pos.read().split(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a297009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x= print(__name__ == '__main__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f47112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "data=[]\n",
    "\n",
    "def text(): \n",
    "\n",
    "    #### This is for fetching the data from given input excel sheet #####\n",
    "    wbx = openpyxl.load_workbook('Input.xlsx')  \n",
    "    wsx = wbx['Sheet1']\n",
    "    \n",
    "    #### for fetching url from sheet into url variable ##### \n",
    "    for i in range (2 , 172):\n",
    "        urllink = (wsx.cell(row=i, column=2).value)\n",
    "    \n",
    "    #pass argument:Headers by passing User-Agent to the request to bypass mod-security error.\n",
    "\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}     \n",
    "        page = requests.get(urllink, headers=headers)\n",
    "    \n",
    "    # apply BeautifulSoup \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "    # fetch title from link\n",
    "        title = soup.find('h1',class_=\"entry-title\").text.replace('\\n',\" \")\n",
    "    \n",
    "    #fetch content from link and remove unwanted punctuation\n",
    "        content = soup.findAll(attrs={'class':'td-post-content'})    \n",
    "        content = content[0].text.replace('\\n',\" \")    \n",
    "        content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    #tokenize data \n",
    "        text_tokens = word_tokenize(content)\n",
    "        \n",
    "    \n",
    "    #removing unecessary stopwords\n",
    "        my_stop_words = stopwords.words('english')\n",
    "        no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "    \n",
    "    #count positive score using positive dictionary \n",
    "        pos_count = \" \".join ([w for w in no_stop_tokens if w in posw])   \n",
    "        pos_count=pos_count.split(\" \")  \n",
    "        Positive_score=len(pos_count)\n",
    "    \n",
    "    #count negative score using negative dictionary\n",
    "        neg_count = \" \".join ([w for w in no_stop_tokens if w in negw])    \n",
    "        neg_count=neg_count.split(\" \")    \n",
    "        Negative_score=len(neg_count)\n",
    "        \n",
    "    #join the filter data after removing stpowords  \n",
    "        filter_content = ' '.join(no_stop_tokens)\n",
    "        \n",
    "    #words count \n",
    "        Word_Count=len(content)\n",
    "        \n",
    "    #Avg_Sentence_Length count \n",
    "        Avg_Sentence_Length = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "    \n",
    "    #calculating fog index using textstat lib\n",
    "        Fog_Index=(textstat.gunning_fog(content))\n",
    "    \n",
    "    #Avg_Number_of_Words_Per_Sentence count\n",
    "        Avg_Number_of_Words_Per_Sentence = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "        Avg_Number_of_Words_Per_Sentence=(sum(Avg_Number_of_Words_Per_Sentence)/len(Avg_Number_of_Words_Per_Sentence))\n",
    "    \n",
    "        Word_Count=len(content)\n",
    "\n",
    "        \n",
    "    #func to calc Complex_Words considering word not ending from \"ed\" or \"es\"\n",
    "        def syllablecount(word):\n",
    "            coun = 0\n",
    "            vowels = \"AEIOUYaeiouy\"\n",
    "            if word[0] in vowels:\n",
    "                coun = coun + 1\n",
    "            for index in range(1, len(word)): \n",
    "                    if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                        coun = coun + 1\n",
    "                        if word.endswith(\"es\"or \"ed\"):\n",
    "                            coun = coun - 1\n",
    "            if coun == 0:\n",
    "                coun = coun + 1\n",
    "            return coun\n",
    "        Complex_Words = syllablecount(content)\n",
    "\n",
    "    \n",
    "    #func to calc proper noun in article with help of tagging from nltk lib\n",
    "        def ProperNounExtractor(text):\n",
    "            cou = 0\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                for (word, tag) in tagged:\n",
    "                    if tag == 'PRP': # If the word is a proper noun\n",
    "                        cou = cou + 1 \n",
    "        \n",
    "            return(cou) \n",
    "        Personal_Pronouns=ProperNounExtractor(content)\n",
    "    \n",
    "\n",
    "    #function for sentiment analysis\n",
    "        def sentiment_analysis(text):\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            return (sentiment.polarity)\n",
    "    \n",
    "        polarity=sentiment_analysis(content)\n",
    "  \n",
    "        def sentiment_analysis(text):\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            return (sentiment.subjectivity)\n",
    "    \n",
    "        subjectivity=sentiment_analysis(content)\n",
    "        \n",
    "        \n",
    "    #method to count average syllable count\n",
    "        word=content.replace(' ','')\n",
    "        syllable_count = 0\n",
    "        for w in word:\n",
    "            \n",
    "            if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "                syllable_count=syllable_count+1\n",
    "\n",
    "        Syllable_Per_Word=(syllable_count/len(content.split()))\n",
    "        \n",
    "    # calculate avg word length\n",
    "        Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "        \n",
    "    # calculate % of complex word\n",
    "        Percentage_of_Complex_Word = Complex_Words / Word_Count * 100\n",
    "    \n",
    "        data.insert(i,[urllink,Positive_score, Negative_score, polarity,subjectivity, Avg_Sentence_Length,Percentage_of_Complex_Word,Fog_Index, Avg_Number_of_Words_Per_Sentence , Complex_Words, Word_Count,Syllable_Per_Word, Personal_Pronouns, Average_Word_Length])\n",
    " \n",
    " \n",
    "if __name__ == '__main__' :\n",
    "    text()\n",
    "        \n",
    "df = pd.DataFrame(data,columns=['url','Positive_score','Negative_score','polarity','subjectivity', 'Avg_Sentence_Length','Percentage_of_Complex_Word', 'Fog_Index', 'Avg_Number_of_Words_Per_Sentence' , 'Complex_Words', 'Word_Count', 'Syllable_Per_Word','Personal_Pronouns', 'Average_Word_Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d8606b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_of_Complex_Word</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Avg_Number_of_Words_Per_Sentence</th>\n",
       "      <th>Complex_Words</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Syllable_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0.143040</td>\n",
       "      <td>0.478514</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>29.160967</td>\n",
       "      <td>289.41</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1279</td>\n",
       "      <td>4386</td>\n",
       "      <td>2.109551</td>\n",
       "      <td>12</td>\n",
       "      <td>5.158708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.177208</td>\n",
       "      <td>0.493776</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>30.125313</td>\n",
       "      <td>257.97</td>\n",
       "      <td>632.0</td>\n",
       "      <td>1202</td>\n",
       "      <td>3990</td>\n",
       "      <td>2.118671</td>\n",
       "      <td>18</td>\n",
       "      <td>5.311709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "      <td>0.141910</td>\n",
       "      <td>0.539460</td>\n",
       "      <td>9654.0</td>\n",
       "      <td>29.886562</td>\n",
       "      <td>726.17</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>3425</td>\n",
       "      <td>11460</td>\n",
       "      <td>2.148559</td>\n",
       "      <td>44</td>\n",
       "      <td>5.351441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077056</td>\n",
       "      <td>0.458329</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>30.182482</td>\n",
       "      <td>179.39</td>\n",
       "      <td>433.0</td>\n",
       "      <td>827</td>\n",
       "      <td>2740</td>\n",
       "      <td>2.166282</td>\n",
       "      <td>6</td>\n",
       "      <td>5.325635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.471385</td>\n",
       "      <td>6191.0</td>\n",
       "      <td>30.504338</td>\n",
       "      <td>493.42</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>2250</td>\n",
       "      <td>7376</td>\n",
       "      <td>2.085925</td>\n",
       "      <td>43</td>\n",
       "      <td>5.066285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>0.420271</td>\n",
       "      <td>7700.0</td>\n",
       "      <td>30.315858</td>\n",
       "      <td>609.48</td>\n",
       "      <td>1514.0</td>\n",
       "      <td>2793</td>\n",
       "      <td>9213</td>\n",
       "      <td>2.058785</td>\n",
       "      <td>32</td>\n",
       "      <td>5.085865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0.102483</td>\n",
       "      <td>0.481812</td>\n",
       "      <td>5254.0</td>\n",
       "      <td>30.347938</td>\n",
       "      <td>384.24</td>\n",
       "      <td>949.0</td>\n",
       "      <td>1884</td>\n",
       "      <td>6208</td>\n",
       "      <td>2.163330</td>\n",
       "      <td>10</td>\n",
       "      <td>5.536354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.486082</td>\n",
       "      <td>5253.0</td>\n",
       "      <td>30.990672</td>\n",
       "      <td>389.71</td>\n",
       "      <td>963.0</td>\n",
       "      <td>1927</td>\n",
       "      <td>6218</td>\n",
       "      <td>2.229491</td>\n",
       "      <td>17</td>\n",
       "      <td>5.454829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057445</td>\n",
       "      <td>0.480637</td>\n",
       "      <td>2878.0</td>\n",
       "      <td>30.420992</td>\n",
       "      <td>238.10</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1055</td>\n",
       "      <td>3468</td>\n",
       "      <td>2.003413</td>\n",
       "      <td>20</td>\n",
       "      <td>4.911263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>0.066522</td>\n",
       "      <td>0.410040</td>\n",
       "      <td>3948.0</td>\n",
       "      <td>29.090523</td>\n",
       "      <td>308.00</td>\n",
       "      <td>760.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4706</td>\n",
       "      <td>2.027632</td>\n",
       "      <td>7</td>\n",
       "      <td>5.194737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  Positive_score  \\\n",
       "0    https://insights.blackcoffer.com/how-is-login-...              16   \n",
       "1    https://insights.blackcoffer.com/how-does-ai-h...              27   \n",
       "2    https://insights.blackcoffer.com/ai-and-its-im...              76   \n",
       "3    https://insights.blackcoffer.com/how-do-deep-l...              14   \n",
       "4    https://insights.blackcoffer.com/how-artificia...              51   \n",
       "..                                                 ...             ...   \n",
       "165  https://insights.blackcoffer.com/role-big-data...              41   \n",
       "166  https://insights.blackcoffer.com/sales-forecas...              36   \n",
       "167  https://insights.blackcoffer.com/detect-data-e...              17   \n",
       "168  https://insights.blackcoffer.com/data-exfiltra...               2   \n",
       "169  https://insights.blackcoffer.com/impacts-of-co...              25   \n",
       "\n",
       "     Negative_score  polarity  subjectivity  Avg_Sentence_Length  \\\n",
       "0                 9  0.143040      0.478514               3673.0   \n",
       "1                 3  0.177208      0.493776               3357.0   \n",
       "2                20  0.141910      0.539460               9654.0   \n",
       "3                 1  0.077056      0.458329               2306.0   \n",
       "4                13  0.063581      0.471385               6191.0   \n",
       "..              ...       ...           ...                  ...   \n",
       "165              18  0.130195      0.420271               7700.0   \n",
       "166              19  0.102483      0.481812               5254.0   \n",
       "167              40  0.050360      0.486082               5253.0   \n",
       "168               7  0.057445      0.480637               2878.0   \n",
       "169              13  0.066522      0.410040               3948.0   \n",
       "\n",
       "     Percentage_of_Complex_Word  Fog_Index  Avg_Number_of_Words_Per_Sentence  \\\n",
       "0                     29.160967     289.41                             712.0   \n",
       "1                     30.125313     257.97                             632.0   \n",
       "2                     29.886562     726.17                            1804.0   \n",
       "3                     30.182482     179.39                             433.0   \n",
       "4                     30.504338     493.42                            1222.0   \n",
       "..                          ...        ...                               ...   \n",
       "165                   30.315858     609.48                            1514.0   \n",
       "166                   30.347938     384.24                             949.0   \n",
       "167                   30.990672     389.71                             963.0   \n",
       "168                   30.420992     238.10                             586.0   \n",
       "169                   29.090523     308.00                             760.0   \n",
       "\n",
       "     Complex_Words  Word_Count  Syllable_Per_Word  Personal_Pronouns  \\\n",
       "0             1279        4386           2.109551                 12   \n",
       "1             1202        3990           2.118671                 18   \n",
       "2             3425       11460           2.148559                 44   \n",
       "3              827        2740           2.166282                  6   \n",
       "4             2250        7376           2.085925                 43   \n",
       "..             ...         ...                ...                ...   \n",
       "165           2793        9213           2.058785                 32   \n",
       "166           1884        6208           2.163330                 10   \n",
       "167           1927        6218           2.229491                 17   \n",
       "168           1055        3468           2.003413                 20   \n",
       "169           1369        4706           2.027632                  7   \n",
       "\n",
       "     Average_Word_Length  \n",
       "0               5.158708  \n",
       "1               5.311709  \n",
       "2               5.351441  \n",
       "3               5.325635  \n",
       "4               5.066285  \n",
       "..                   ...  \n",
       "165             5.085865  \n",
       "166             5.536354  \n",
       "167             5.454829  \n",
       "168             4.911263  \n",
       "169             5.194737  \n",
       "\n",
       "[170 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('linkd.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a84466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
